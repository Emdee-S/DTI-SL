{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdaee98-c950-425e-b5b1-08efea97a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch==2.5.1 in /opt/conda/lib/python3.11/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.5.1) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e5b4d0-ef13-4173-aa1f-58cd7cf730b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgl==1.1.2 in /opt/conda/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (1.15.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (4.66.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from dgl==1.1.2) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->dgl==1.1.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->dgl==1.1.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->dgl==1.1.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->dgl==1.1.2) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dgl==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eddbcc5-7f93-4899-b0a3-14e183601ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata==0.9.0 in /opt/conda/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.11/site-packages (from torchdata==0.9.0) (2.2.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torchdata==0.9.0) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/lib/python3.11/site-packages (from torchdata==0.9.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2->torchdata==0.9.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2->torchdata==0.9.0) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torchdata==0.9.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torchdata==0.9.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torchdata==0.9.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2->torchdata==0.9.0) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchdata==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f15bf1f-d9cc-49e7-a4cc-8f89652abadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ae7f03-110b-457a-93a1-7dd331bc9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgllife in /opt/conda/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.11/site-packages (from dgllife) (1.6.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from dgllife) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.11/site-packages (from dgllife) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from dgllife) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from dgllife) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from dgllife) (1.15.3)\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.11/site-packages (from dgllife) (3.3)\n",
      "Requirement already satisfied: hyperopt in /opt/conda/lib/python3.11/site-packages (from dgllife) (0.2.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from dgllife) (1.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.22.0->dgllife) (2024.7.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=0.22.2->dgllife) (3.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from hyperopt->dgllife) (1.16.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.11/site-packages (from hyperopt->dgllife) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from hyperopt->dgllife) (3.1.1)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.11/site-packages (from hyperopt->dgllife) (0.10.9.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->dgllife) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->dgllife) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->dgllife) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dgllife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61fe43c-39bd-4ae5-a203-83295c489d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /opt/conda/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rdkit) (2.1.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from rdkit) (11.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0f7e23-30d1-4c90-b34a-0ff2246f3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in /opt/conda/lib/python3.11/site-packages (1.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed019aaf-2149-4e22-8070-de5b329aaecf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA version: 12.4\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA L40\n",
      "Total memory (GB): 47.58\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_available)\n",
    "\n",
    "if cuda_available:\n",
    "    device = torch.cuda.get_device_properties(0)\n",
    "    print(\"GPU name:\", device.name)\n",
    "    print(\"Total memory (GB):\", round(device.total_memory / 1e9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ac094f-c76d-4350-a4c4-cc57155d06d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "from dgllife.model.gnn import GCN\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.weight_norm import weight_norm\n",
    "from typing import Optional\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4e60f8-93a7-4acd-bc10-48757c003150",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arguments for general hyperparameters, proteins, and smiles, going to args.py \n",
    "class Args:\n",
    "    n_epochs   = 100\n",
    "    batch_size = 64\n",
    "    lr         = 0.0001\n",
    "    seed       = 2048\n",
    "    n_cpu      = 2\n",
    "    shuffle    = True\n",
    "    reg        = 5e-5\n",
    "    drop       = 0.1\n",
    "    S          = 'human'\n",
    "    T          = 'biosnap'\n",
    "\n",
    "    # Decoder args\n",
    "    mlp_in_dim = 256\n",
    "    mlp_hidden_dim = 512\n",
    "    mlp_out_dim = 128\n",
    "    binary = 1\n",
    "args = Args()\n",
    "\n",
    "# there are many more arguments to be added for both smiles and proteins later\n",
    "class Prot_Args:\n",
    "    max = 1000\n",
    "    encode_dim = 512\n",
    "    layers = 3\n",
    "    num_heads = 8\n",
    "    embedding_dim = 128\n",
    "    num_filters = [128, 128, 128]\n",
    "    filter_size = [3, 6, 9]\n",
    "    padding = True\n",
    "\n",
    "    # Transformer-related settings\n",
    "    feed_forward_expansion_factor = 4\n",
    "    feed_forward_dropout_p = 0.1\n",
    "    attention_dropout_p = 0.1\n",
    "    conv_dropout_p = 0.1\n",
    "    conv_kernel_size = 3\n",
    "    \n",
    "prot_args = Prot_Args()\n",
    "\n",
    "class Smiles_Args:\n",
    "    max_nodes = 290\n",
    "    drug_in = 75\n",
    "    embedding_dim =  128\n",
    "    hidden_layers = 128\n",
    "    padding = True\n",
    "    \n",
    "smiles_args = Smiles_Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c543071-4cb3-454d-af61-90f6607f34b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/DTI-SL\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d13b1d-0b22-4f94-b637-0b56728deeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# home/jovyan is from JupyterHub, going to loader.py\n",
    "S_dir = Path(\"/home/jovyan/DTI-SL/datasets\") / args.S # domain here\n",
    "\n",
    "train = pd.read_csv(S_dir / \"train.csv\")[:500]\n",
    "val   = pd.read_csv(S_dir / \"val.csv\")[:100]\n",
    "test  = pd.read_csv(S_dir / \"test.csv\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b142888e-5377-47e6-977c-ff9c469ab353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3),\n",
       " (100, 3),\n",
       " (100, 3),\n",
       " Index(['SMILES', 'Protein', 'Y'], dtype='object'),\n",
       "                                               SMILES  \\\n",
       " 0  CC(C)[C@H](NS(=O)(=O)C1=CC=C(C=C1)C2=CC=C(C=C2...   \n",
       " 1  [O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2].[O-2...   \n",
       " 2              CC12CCC3C(C1CCC2=O)CCC4=CC(=O)CCC34CO   \n",
       " 3  C1CN(CCC1(C2=CC=C(C=C2)Cl)O)CCCC(=O)C3=CC=C(C=...   \n",
       " 4  C1C[C@@H]2CN[C@H](C[C@@H]2C[C@@H]1CCC3=NNN=N3)...   \n",
       " \n",
       "                                              Protein  Y  \n",
       " 0  MILLTFSTGRRLDFVHHSGVFFLQTLLWILCATVCGTEQYFNVEVW...  1  \n",
       " 1  MLPSASRERPGYRAGVAAPDLLDPKSAAQNSKPRLSFSTKPTVLAS...  0  \n",
       " 2  MWSWKCLLFWAVLVTATLCTARPSPTLPEQAQPWGAPVEVESFLVH...  0  \n",
       " 3  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...  1  \n",
       " 4  MAEDGEEAEFHFAALYISGQWPRLRADTDLQRLGSSAMAPSRKFFV...  0  )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape, train.columns, train.head() # 3 for drug,protein,label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2848b3-675c-4fa5-9ca4-6c39ecfaef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from functools import partial\n",
    "from dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53efa39-2e0d-4fb0-aaf0-62a6b1b44c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce234906-d878-400d-a017-17f4b7d2b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the protein matrix, going to loader.py\n",
    "# ---------- amino-acid dictionary ----------\n",
    "amino_dict = {\"B\": 1, \"A\": 2, \"C\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
    "              \"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
    "              \"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
    "              \"U\": 19, \"T\": 20, \"W\": 21, \"V\": 22, \"X\": 23, \"Z\": 24, \"Y\": 25}\n",
    "\n",
    "# ---------- helper: encode one protein ----------\n",
    "def encode_protein(seq: str, max_len: int):\n",
    "    encoded = [amino_dict.get(res, 0) for res in seq[:max_len]]\n",
    "    if len(encoded) < max_len:\n",
    "        encoded.extend([0] * (max_len - len(encoded)))\n",
    "    return np.asarray(encoded, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0500f19-dbe9-4d94-a390-e4fd31024d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class, going to loader.py\n",
    "class DrugProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A lightweight Dataset that:\n",
    "      • stores pre-encoded proteins (np.ndarray of shape [N, max_len])\n",
    "      • builds/pads drug graphs on demand\n",
    "      • returns tensors ready for model input\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame,\n",
    "                 prot_args: Prot_Args,\n",
    "                 smiles_args: Smiles_Args):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.p_max = prot_args.max # max is 1000 amino acids\n",
    "        self.d_max = smiles_args.max_nodes # max is 290 nodes in the graph\n",
    "\n",
    "        # — pre-encode every protein once —\n",
    "        self.protein_int = np.stack(\n",
    "            df[\"Protein\"].apply(lambda s: encode_protein(s, self.p_max)).values\n",
    "        )\n",
    "\n",
    "        # — drug graph helpers —\n",
    "        self.atom_featurizer = CanonicalAtomFeaturizer()\n",
    "        self.bond_featurizer = CanonicalBondFeaturizer(self_loop=True)\n",
    "        self.bigraph_fn = partial(smiles_to_bigraph, add_self_loop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- DRUG / SMILES --------\n",
    "        g = self.bigraph_fn(\n",
    "            smiles=row[\"SMILES\"],\n",
    "            node_featurizer=self.atom_featurizer,\n",
    "            edge_featurizer=self.bond_featurizer,\n",
    "        )\n",
    "\n",
    "        # real-node indicator before padding\n",
    "        n_real = g.num_nodes()\n",
    "        real_node_bit = torch.zeros(n_real, 1)\n",
    "        g.ndata[\"h\"] = torch.cat([g.ndata.pop(\"h\"), real_node_bit], dim=1) # h gets node features, e for bond features\n",
    "\n",
    "        # pad to self.d_max virtual nodes\n",
    "        n_fake = self.d_max - n_real\n",
    "        if n_fake < 0:\n",
    "            raise ValueError(f\"SMILES string at index {idx} has {n_real} atoms \"\n",
    "                             f\"which exceeds max_nodes={self.d_max}. \"\n",
    "                             \"Either increase Smiles_Args.max_nodes or drop this entry.\")\n",
    "        if n_fake:\n",
    "            virtual_feats = torch.cat([torch.zeros(n_fake, 74),\n",
    "                                       torch.ones(n_fake, 1)], dim=1)\n",
    "            g.add_nodes(n_fake, {\"h\": virtual_feats})\n",
    "\n",
    "        # -------- PROTEIN --------\n",
    "        prot_int = torch.tensor(self.protein_int[idx], dtype=torch.long)     # [max_len]\n",
    "        mask = (prot_int != 0).float()                                       # 1 for real, 0 for pad\n",
    "\n",
    "        # -------- LABEL --------\n",
    "        y = torch.tensor(row[\"Y\"], dtype=torch.float32)\n",
    "\n",
    "        return g, prot_int, mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f94eca7-75bf-4638-b401-88e32ffd0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=290, num_edges=75,\n",
      "      ndata_schemes={'h': Scheme(shape=(75,), dtype=torch.float32)}\n",
      "      edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}) torch.Size([1000]) tensor(432.) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([290, 75])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test block\n",
    "prot_args   = Prot_Args()\n",
    "smiles_args = Smiles_Args()\n",
    "\n",
    "train_data = DrugProteinDataset(train, prot_args, smiles_args)\n",
    "val_data   = DrugProteinDataset(val,   prot_args, smiles_args)\n",
    "test_data  = DrugProteinDataset(test,  prot_args, smiles_args)\n",
    "\n",
    "# sanity check, this will show nodes, edges, protein size for a single instance \n",
    "g, prot_int, mask, y = train_data[50]\n",
    "print(g, prot_int.size(), mask.sum(), y)\n",
    "g.ndata[\"h\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b680487-24f6-4511-96c2-9797bb271e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN, drug encoder block\n",
    "class DrugGCN(nn.Module):\n",
    "    def __init__(self, in_feats,\n",
    "                 dim_embedding=128, \n",
    "                 padding=True,\n",
    "                 hidden_feats=None, # apparently the DGL library can fill defaults for layers to be the same number as dim, 128x128x128\n",
    "                 activation=None): # activation default is relu for GCNs, imported\n",
    "        super(DrugGCN, self).__init__()\n",
    "\n",
    "        self.init_transform = nn.Linear(in_feats, dim_embedding, bias=False)\n",
    "\n",
    "        if padding:\n",
    "            with torch.no_grad():\n",
    "                self.init_transform.weight[-1].fill_(0)  # zero out mask bit effect\n",
    "\n",
    "        # Use DGL-LifeSci's GCN here directly\n",
    "        self.gnn = GCN(in_feats=dim_embedding,\n",
    "                       hidden_feats=hidden_feats,\n",
    "                       activation=activation)\n",
    "\n",
    "        self.output_feats = hidden_feats[-1]\n",
    "\n",
    "    def forward(self, batch_graph):\n",
    "        node_feats = batch_graph.ndata.pop('h')              # [∑N_i, in_feats]\n",
    "        node_feats = self.init_transform(node_feats)         # [∑N_i, dim_embedding]\n",
    "        node_feats = self.gnn(batch_graph, node_feats)       # [∑N_i, output_feats]\n",
    "        batch_size = batch_graph.batch_size\n",
    "        node_feats = node_feats.view(batch_size, -1, self.output_feats)\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c789ef03-b778-4b52-bb09-6b5cea5bf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNTrans, protein encoder block\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, b = x.chunk(2, dim=self.dim)\n",
    "        return a * torch.sigmoid(b)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.pos_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.u_bias = nn.Parameter(torch.randn(num_heads, self.d_head))\n",
    "        self.v_bias = nn.Parameter(torch.randn(num_heads, self.d_head))\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x, pos_enc, mask=None):\n",
    "        B, L, _ = x.size()\n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        p = self.pos_proj(pos_enc).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        content_score = torch.matmul(q + self.u_bias.unsqueeze(1), k.transpose(-2, -1))\n",
    "        pos_score = torch.matmul(q + self.v_bias.unsqueeze(1), p.transpose(-2, -1))\n",
    "        score = (content_score + self._relative_shift(pos_score)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
    "\n",
    "        attn = F.softmax(score, dim=-1)\n",
    "        context = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, L, -1)\n",
    "        return self.out_proj(context)\n",
    "\n",
    "    def _relative_shift(self, x):\n",
    "        B, H, L1, L2 = x.size()\n",
    "        zero_pad = torch.zeros((B, H, L1, 1), device=x.device, dtype=x.dtype)\n",
    "        x_padded = torch.cat([zero_pad, x], dim=-1)\n",
    "        x_padded = x_padded.view(B, H, L2 + 1, L1)\n",
    "        return x_padded[:, :, 1:].view(B, H, L1, L2)\n",
    "\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * expansion),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * expansion, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Conv1d(d_model, d_model * 2, kernel_size=1),\n",
    "            Swish(),\n",
    "            nn.Conv1d(d_model * 2, d_model * 2, kernel_size, padding=(kernel_size - 1) // 2),\n",
    "            GLU(dim=1),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.seq(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class CNNTransBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, kernel_size=5, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.attn = RelativeMultiHeadAttention(d_model, num_heads)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.conv = ConvModule(d_model, kernel_size, dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        pos = self.pos_enc(x)\n",
    "        x = self.attn(self.norm(x), pos, mask) + x\n",
    "        x = self.conv(x) + x\n",
    "        return 0.5 * self.ff(x) + 0.5 * x\n",
    "\n",
    "\n",
    "class ProteinCNNTrans(nn.Module):\n",
    "    def __init__(self, max_len=1000, encoder_dim=128, num_layers=3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            CNNTransBlock(d_model=encoder_dim, max_len=max_len, **kwargs)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_ff = FeedForwardModule(encoder_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return 0.5 * self.final_ff(x) + 0.5 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6321749e-5485-4566-a548-d0e6b95f5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder block\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, binary=1):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(out_dim)\n",
    "        self.fc4 = nn.Linear(out_dim, binary)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.bn3(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d584bf-1cd7-499b-a1b1-5191a2fc194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_args = Prot_Args()\n",
    "smiles_args = Smiles_Args()\n",
    "args = Args()\n",
    "\n",
    "class DTIModel(nn.Module):\n",
    "    def __init__(self, smiles_args, prot_args, **config):\n",
    "        super(DTIModel, self).__init__()\n",
    "\n",
    "        self.drug_in = smiles_args.drug_in\n",
    "        self.embedding_dim = smiles_args.embedding_dim\n",
    "        self.max_nodes = smiles_args.max_nodes\n",
    "        self.padding = smiles_args.padding\n",
    "        self.hidden_layers = smiles_args.hidden_layers \n",
    "\n",
    "         # Drug feature extractor using Graph Convolutional Network (GCN)\n",
    "        self.drug_encoder = DrugGCN(\n",
    "            in_feats=self.drug_in,\n",
    "            dim_embedding=self.embedding_dim,\n",
    "            hidden_feats=[self.hidden_layers] * 3, # three layers GCN 128x128x128\n",
    "            activation=[F.relu] * 3,\n",
    "            padding=self.padding\n",
    "        )\n",
    "\n",
    "        # Protein feature encoder combining CNN and Transformer\n",
    "        self.protein_encoder = ProteinCNNTrans(\n",
    "            max_len=prot_args.max,\n",
    "            encoder_dim=prot_args.embedding_dim,\n",
    "            num_layers=prot_args.layers,\n",
    "            num_heads=prot_args.num_heads,\n",
    "            kernel_size=prot_args.conv_kernel_size,\n",
    "            dropout=prot_args.conv_dropout_p\n",
    "        )\n",
    "        \n",
    "         # MLP decoder for the final classification output\n",
    "        self.mlp_classifier = MLPClassifier(\n",
    "            args.mlp_in_dim,  # Input dimension for MLP\n",
    "            args.mlp_hidden_dim,  # Hidden layer dimension for MLP\n",
    "            args.mlp_out_dim,  # Output dimension for MLP\n",
    "            args.binary  # Binary classification flag for MLP output\n",
    "        )\n",
    "\n",
    "        self.protein_embed = nn.Embedding(26, 128, padding_idx=0)  # 26 possible amino acids, embedded into 128-dimensional space\n",
    "\n",
    "        self.mix_attention_layer = nn.MultiheadAttention(128, 4)  # 128-dimensional input, 4 attention heads\n",
    "\n",
    "        # Max pooling layers for drug and protein feature extraction\n",
    "        self.Drug_max_pool = nn.MaxPool1d(290)  # Pooling layer for drug features (max pool over 290 values)\n",
    "        self.Protein_max_pool = nn.MaxPool1d(1000)  # Pooling layer for protein features (max pool over 1000 values)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout with a probability of 10%\n",
    "\n",
    "    def forward(self, bg_d, v_p, protein_mask, mode=\"train\"):\n",
    "        # Process drug graph through molecular GCN feature extractor\n",
    "        v_d = self.drug_encoder(bg_d)\n",
    "        \n",
    "        # Embed protein sequences using the protein embedding layer\n",
    "        v_p = self.protein_embed(v_p.long().to(device))  # Convert protein indices to embeddings\n",
    "        protein_mask = protein_mask.long().to(device)  # Protein mask for attention\n",
    "        \n",
    "        # Process protein embeddings through the protein feature encoder\n",
    "        v_p = self.protein_encoder(v_p, protein_mask)\n",
    "\n",
    "        # Prepare for attention by permuting the dimensions of drug and protein features\n",
    "        drugConv = v_d.permute(0, 2, 1)  # Permute for attention processing\n",
    "        proteinConv = v_p.permute(0, 2, 1)  # Permute for attention processing\n",
    "        \n",
    "        # Prepare drug and protein for attention mechanism (Q, K, V are query, key, value)\n",
    "        drug_QKV = drugConv.permute(2, 0, 1)  # Query for drug\n",
    "        protein_QKV = proteinConv.permute(2, 0, 1)  # Query for protein\n",
    "        \n",
    "        # Apply multi-head attention between drug and protein\n",
    "        drug_att, _ = self.mix_attention_layer(drug_QKV, protein_QKV, protein_QKV)\n",
    "        protein_att, _ = self.mix_attention_layer(protein_QKV, drug_QKV, drug_QKV)\n",
    "\n",
    "        # Permute back after attention to get the correct shape\n",
    "        drug_att = drug_att.permute(1, 2, 0)\n",
    "        protein_att = protein_att.permute(1, 2, 0)\n",
    "\n",
    "        # Combine original and attended features\n",
    "        drugConv = drugConv * 0.5 + drug_att * 0.5\n",
    "        proteinConv = proteinConv * 0.5 + protein_att * 0.5\n",
    "\n",
    "        # Apply max pooling to both drug and protein features\n",
    "        drugConv = self.Drug_max_pool(drugConv).squeeze(2)\n",
    "        proteinConv = self.Protein_max_pool(proteinConv).squeeze(2)\n",
    "\n",
    "        # Concatenate drug and protein attended features and apply dropout\n",
    "        result = torch.cat((drug_att, protein_att), dim=-1)\n",
    "        pair = torch.cat([drugConv, proteinConv], dim=1)\n",
    "        pair = self.dropout1(pair)\n",
    "\n",
    "        # Pass concatenated features through MLP classifier\n",
    "        score = self.mlp_classifier(pair)\n",
    "\n",
    "        # Return results based on mode (train or eval)\n",
    "        if mode == \"train\":\n",
    "            return v_d, v_p, pair, score  # Return all outputs for training\n",
    "        elif mode == \"eval\":\n",
    "            return v_d, v_p, score, result  # Return simplified outputs for evaluation\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6050e483-5d9c-4ee7-9fe8-f3993433e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "args = Args()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e62981-9cc9-48c2-a1ec-6798c1452ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDGL CUDA check failed:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdgl_has_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mdgl_has_cuda\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdgl_has_cuda\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def dgl_has_cuda():\n",
    "    if not torch.cuda.is_available():\n",
    "        return False\n",
    "    try:\n",
    "        g = dgl.graph(([0], [1]))\n",
    "        g = g.to('cuda:0')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"DGL CUDA check failed:\", e)\n",
    "        return False\n",
    "\n",
    "if dgl_has_cuda():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5213c06-8c84-40ca-9431-581dd151ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')  # FORCE CPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21417e95-9f46-4693-ae6f-4258605c4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    graphs, prot_seqs, masks, labels = map(list, zip(*batch))\n",
    "    bg = dgl.batch(graphs)  # stays on CPU\n",
    "\n",
    "    prot_seqs = torch.stack(prot_seqs)\n",
    "    masks = torch.stack(masks)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return bg, prot_seqs, masks, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a5fed61-c63f-4cdd-a185-f572c28210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_dir = Path(\"/home/jovyan/DTI-SL/datasets\") / args.S\n",
    "\n",
    "train_df = pd.read_csv(S_dir / \"train.csv\")\n",
    "val_df   = pd.read_csv(S_dir / \"val.csv\")\n",
    "test_df  = pd.read_csv(S_dir / \"test.csv\")\n",
    "\n",
    "train_data = DrugProteinDataset(train_df, prot_args, smiles_args)\n",
    "val_data   = DrugProteinDataset(val_df, prot_args, smiles_args)\n",
    "test_data  = DrugProteinDataset(test_df, prot_args, smiles_args)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=args.shuffle, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_data, batch_size=args.batch_size, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_data, batch_size=args.batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "499cb560-784e-4035-997c-d58fcdcb77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DTIModel(smiles_args, prot_args).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.reg)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "201034f7-abaf-43d3-b8c7-1977ee700837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    model.to(device)  # moves model to CPU\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for g_batch, prot_int, mask, y in train_loader:\n",
    "            g_batch = g_batch.cpu()  # DGL graph on CPU\n",
    "            prot_int = prot_int.to(device)\n",
    "            mask = mask.to(device)\n",
    "            y = y.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            _, _, _, output = model(g_batch, prot_int, mask, mode=\"train\")\n",
    "            loss = criterion(output.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "            preds = (output.squeeze() > 0.5).float()\n",
    "            train_correct += (preds == y).sum().item()\n",
    "            total_train += y.size(0)\n",
    "\n",
    "        train_loss /= total_train\n",
    "        train_acc = train_correct / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for g_batch, prot_int, mask, y in val_loader:\n",
    "                g_batch = g_batch.cpu()\n",
    "                prot_int = prot_int.to(device)\n",
    "                mask = mask.to(device)\n",
    "                y = y.to(device).float()\n",
    "\n",
    "                _, _, _, output = model(g_batch, prot_int, mask, mode=\"eval\")\n",
    "                loss = criterion(output.squeeze(), y)\n",
    "\n",
    "                val_loss += loss.item() * y.size(0)\n",
    "                preds = (output.squeeze() > 0.5).float()\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                total_val += y.size(0)\n",
    "\n",
    "        val_loss /= total_val\n",
    "        val_acc = val_correct / total_val\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cde4d170-cf28-4371-8ec4-639f272894db",
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "There are 0-in-degree nodes in the graph, output for those nodes will be invalid. This is harmful for some applications, causing silent performance regression. Adding self-loop on the input graph by calling `g = dgl.add_self_loop(g)` will resolve the issue. Setting ``allow_zero_in_degree`` to be `True` when constructing this module will suppress the check and let the code run.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m _, _, _, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprot_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), y)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[28], line 55\u001b[0m, in \u001b[0;36mDTIModel.forward\u001b[0;34m(self, bg_d, v_p, protein_mask, mode)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, bg_d, v_p, protein_mask, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Process drug graph through molecular GCN feature extractor\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     v_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrug_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg_d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Embed protein sequences using the protein embedding layer\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     v_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_embed(v_p\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device))  \u001b[38;5;66;03m# Convert protein indices to embeddings\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mDrugGCN.forward\u001b[0;34m(self, batch_graph)\u001b[0m\n\u001b[1;32m     24\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m batch_graph\u001b[38;5;241m.\u001b[39mndata\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)              \u001b[38;5;66;03m# [∑N_i, in_feats]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_transform(node_feats)         \u001b[38;5;66;03m# [∑N_i, dim_embedding]\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# [∑N_i, output_feats]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_graph\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     28\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m node_feats\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_feats)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dgllife/model/gnn/gcn.py:235\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, g, feats)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update node representations.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m      hidden_sizes[-1] in initialization.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_layers:\n\u001b[0;32m--> 235\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feats\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dgllife/model/gnn/gcn.py:101\u001b[0m, in \u001b[0;36mGCNLayer.forward\u001b[0;34m(self, g, feats)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, feats):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update node representations.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m        * M2 is the output node feature size, which must match out_feats in initialization\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     new_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m    103\u001b[0m         res_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_connection(feats))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dgl/nn/pytorch/conv/graphconv.py:408\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_zero_in_degree:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (graph\u001b[38;5;241m.\u001b[39min_degrees() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are 0-in-degree nodes in the graph, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput for those nodes will be invalid. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is harmful for some applications, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausing silent performance regression. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding self-loop on the input graph by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `g = dgl.add_self_loop(g)` will resolve \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe issue. Setting ``allow_zero_in_degree`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be `True` when constructing this module will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuppress the check and let the code run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         )\n\u001b[1;32m    419\u001b[0m aggregate_fn \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39mcopy_u(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mDGLError\u001b[0m: There are 0-in-degree nodes in the graph, output for those nodes will be invalid. This is harmful for some applications, causing silent performance regression. Adding self-loop on the input graph by calling `g = dgl.add_self_loop(g)` will resolve the issue. Setting ``allow_zero_in_degree`` to be `True` when constructing this module will suppress the check and let the code run."
     ]
    }
   ],
   "source": [
    "train_and_eval(model, train_loader, val_loader, optimizer, criterion, epochs=args.n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d48684-ce12-44c6-9e88-857ae21edfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
